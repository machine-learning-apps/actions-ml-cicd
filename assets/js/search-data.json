{
  
  "0": {
    "title": "",
    "content": "404 . Page not found :( . The requested page could not be found. .",
    "url": "https://machine-learning-apps.github.io/actions-ml-cicd/404.html",
    "relUrl": "/404.html"
  }
  ,"1": {
    "title": "Dashboard",
    "content": "df = pd.read_json(movies) # load movies data genres = df[&#39;Major_Genre&#39;].unique() # get unique field values genres = list(filter(lambda d: d is not None, genres)) # filter out None values genres.sort() # sort alphabetically . mpaa = [&#39;G&#39;, &#39;PG&#39;, &#39;PG-13&#39;, &#39;R&#39;, &#39;NC-17&#39;, &#39;Not Rated&#39;] . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Fun facts: The PG-13 rating didn&#39;t exist when the movies Jaws and Jaws 2 were released. The first film to receive a PG-13 rating was 1984&#39;s Red Dawn. . Using Visualizations as Dynamic Queries&#182; . Though standard interface widgets show the possible query parameter values, they do not visualize the distribution of those values. We might also wish to use richer interactions, such as multi-value or interval selections, rather than input widgets that select only a single value at a time. . To address these issues, we can author additional charts to both visualize data and support dynamic queries. Let&#39;s add a histogram of the count of films per year and use an interval selection to dynamically highlight films over selected time periods. . Interact with the year histogram to explore films from different time periods. Do you seen any evidence of sampling bias across the years? (How do year and critics&#39; ratings relate?) . The years range from 1930 to 2040! Are future films in pre-production, or are there &quot;off-by-one century&quot; errors? Also, depending on which time zone you&#39;re in, you may see a small bump in either 1969 or 1970. Why might that be? (See the end of the notebook for an explanation!) . brush = alt.selection_multi( on=&#39;mouseover&#39;, encodings=[&#39;x&#39;] # limit selection to x-axis (year) values ) # dynamic query histogram years = alt.Chart(movies).mark_bar().add_selection( brush ).encode( alt.X(&#39;year(Release_Date):T&#39;, title=&#39;Films by Release Year&#39;), alt.Y(&#39;count():Q&#39;, title=None), opacity=alt.condition(brush, alt.value(0.75), alt.value(0.05)) ).properties( width=650, height=50 ) # scatter plot, modify opacity based on selection ratings = alt.Chart(movies).mark_circle().encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(brush, alt.value(0.75), alt.value(0.05)) ).properties( width=650, height=400 ) alt.vconcat(years, ratings).properties(spacing=5) . Details on Demand&#182; . Once we spot points of interest within a visualization, we often want to know more about them. Details-on-demand refers to interactively querying for more information about selected values. Tooltips are one useful means of providing details on demand. However, tooltips typically only show information for one data point at a time. How might we show more? . The movie ratings scatterplot includes a number of potentially interesting outliers where the Rotten Tomatoes and IMDB ratings disagree. Let&#39;s create a plot that allows us to interactively select points and show their labels. . Mouse over points in the scatter plot below to see a highlight and title label. Shift-click points to make annotations persistent and view multiple labels at once. Which movies are loved by Rotten Tomatoes critics, but not the general audience on IMDB (or vice versa)? See if you can find possible errors, where two different movies with the same name were accidentally combined! . hover = alt.selection_single( on=&#39;mouseover&#39;, # select on mouseover nearest=True, # select nearest point to mouse cursor empty=&#39;none&#39; # empty selection should match nothing ) click = alt.selection_multi( empty=&#39;none&#39; # empty selection matches no points ) # scatter plot encodings shared by all marks plot = alt.Chart().mark_circle().encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39; ) # shared base for new layers base = plot.transform_filter( # logical OR is supported by Vega-Lite, nice syntax still needed for Altair {&#39;or&#39;: [hover, click]} # filter to points in either selection ) # layer scatter plot points, halo annotations, and title labels alt.layer( plot.add_selection(hover).add_selection(click), base.mark_point(size=100, stroke=&#39;firebrick&#39;, strokeWidth=1), base.mark_text(dx=4, dy=-8, align=&#39;right&#39;, stroke=&#39;white&#39;, strokeWidth=2).encode(text=&#39;Title:N&#39;), base.mark_text(dx=4, dy=-8, align=&#39;right&#39;).encode(text=&#39;Title:N&#39;), data=movies ).properties( width=600, height=450 ) . Using selections and layers, we can realize a number of different designs for details on demand! For example, here is a log-scaled time series of technology stock prices, annotated with a guideline and labels for the date nearest the mouse cursor: . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=5).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Putting into action what we&#39;ve learned so far: can you modify the movie scatter plot above (the one with the dynamic query over years) to include a rule mark that shows the average IMDB (or Rotten Tomatoes) rating for the data contained within the year interval selection? . Brushing &amp; Linking, Revisited&#182; . Earlier in this notebook we saw an example of brushing &amp; linking: using a dynamic query histogram to highlight points in a movie rating scatter plot. Here, we&#39;ll visit some additional examples involving linked selections. . Returning to the cars dataset, we can use the repeat operator to build a scatter plot matrix (SPLOM) that shows associations between mileage, acceleration, and horsepower. We can define an interval selection and include it within our repeated scatter plot specification to enable linked selections among all the plots. . Click and drag in any of the plots below to perform brushing &amp; linking! . brush = alt.selection_interval( resolve=&#39;global&#39; # resolve all selections to a single global instance ) alt.Chart(cars).mark_circle().add_selection( brush ).encode( alt.X(alt.repeat(&#39;column&#39;), type=&#39;quantitative&#39;), alt.Y(alt.repeat(&#39;row&#39;), type=&#39;quantitative&#39;), color=alt.condition(brush, &#39;Cylinders:O&#39;, alt.value(&#39;grey&#39;)), opacity=alt.condition(brush, alt.value(0.8), alt.value(0.1)) ).properties( width=140, height=140 ).repeat( column=[&#39;Acceleration&#39;, &#39;Horsepower&#39;, &#39;Miles_per_Gallon&#39;], row=[&#39;Miles_per_Gallon&#39;, &#39;Horsepower&#39;, &#39;Acceleration&#39;] ) . Cross-Filtering&#182; . The brushing &amp; linking examples we&#39;ve looked at all use conditional encodings, for example to change opacity values in response to a selection. Another option is to use a selection defined in one view to filter the content of another view. . Let&#39;s build a collection of histograms for the flights dataset: arrival delay (how early or late a flight arrives, in minutes), distance flown (in miles), and time of departure (hour of the day). We&#39;ll use the repeat operator to create the histograms, and add an interval selection for the x axis with brushes resolved via intersection. . In particular, each histogram will consist of two layers: a gray background layer and a blue foreground layer, with the foreground layer filtered by our intersection of brush selections. The result is a cross-filtering interaction across the three charts! . Drag out brush intervals in the charts below. As you select flights with longer or shorter arrival delays, how do the distance and time distributions respond? . brush = alt.selection_interval( encodings=[&#39;x&#39;], resolve=&#39;intersect&#39; ); hist = alt.Chart().mark_bar().encode( alt.X(alt.repeat(&#39;row&#39;), type=&#39;quantitative&#39;, bin=alt.Bin(maxbins=100, minstep=1), # up to 100 bins axis=alt.Axis(format=&#39;d&#39;, titleAnchor=&#39;start&#39;) # integer format, left-aligned title ), alt.Y(&#39;count():Q&#39;, title=None) # no y-axis title ) alt.layer( hist.add_selection(brush).encode(color=alt.value(&#39;lightgrey&#39;)), hist.transform_filter(brush) ).properties( width=900, height=100 ).repeat( row=[&#39;delay&#39;, &#39;distance&#39;, &#39;time&#39;], data=flights ).transform_calculate( delay=&#39;datum.delay &lt; 180 ? datum.delay : 180&#39;, # clamp delays &gt; 3 hours time=&#39;hours(datum.date) + minutes(datum.date) / 60&#39; # fractional hours ).configure_view( stroke=&#39;transparent&#39; # no outline ) . By cross-filtering you can observe that delayed flights are more likely to depart at later hours. This phenomenon is familiar to frequent fliers: a delay can propagate through the day, affecting subsequent travel by that plane. For the best odds of an on-time arrival, book an early flight! . The combination of multiple views and interactive selections can enable valuable forms of multi-dimensional reasoning, turning even basic histograms into powerful input devices for asking questions of a dataset! .",
    "url": "https://machine-learning-apps.github.io/actions-ml-cicd/docs/reports/Dashboard/",
    "relUrl": "/docs/reports/Dashboard/"
  }
  ,"2": {
    "title": "EDA",
    "content": "# Python ≥3.5 is required import sys assert sys.version_info &gt;= (3, 5) # Scikit-Learn ≥0.20 is required import sklearn assert sklearn.__version__ &gt;= &quot;0.20&quot; # Common imports import numpy as np import os # To plot pretty figures %matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt mpl.rc(&#39;axes&#39;, labelsize=14) mpl.rc(&#39;xtick&#39;, labelsize=12) mpl.rc(&#39;ytick&#39;, labelsize=12) # Where to save the figures PROJECT_ROOT_DIR = &quot;.&quot; CHAPTER_ID = &quot;end_to_end_project&quot; IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, &quot;images&quot;, CHAPTER_ID) os.makedirs(IMAGES_PATH, exist_ok=True) def save_fig(fig_id, tight_layout=True, fig_extension=&quot;png&quot;, resolution=300): path = os.path.join(IMAGES_PATH, fig_id + &quot;.&quot; + fig_extension) print(&quot;Saving figure&quot;, fig_id) if tight_layout: plt.tight_layout() plt.savefig(path, format=fig_extension, dpi=resolution) # Ignore useless warnings (see SciPy issue #5998) import warnings warnings.filterwarnings(action=&quot;ignore&quot;, message=&quot;^internal gelsd&quot;) . Preview of Dataset&#182; . Available at https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.tgz . import os import tarfile import urllib DOWNLOAD_ROOT = &quot;https://raw.githubusercontent.com/ageron/handson-ml2/master/&quot; HOUSING_PATH = os.path.join(&quot;datasets&quot;, &quot;housing&quot;) HOUSING_URL = DOWNLOAD_ROOT + &quot;datasets/housing/housing.tgz&quot; def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH): if not os.path.isdir(housing_path): os.makedirs(housing_path) tgz_path = os.path.join(housing_path, &quot;housing.tgz&quot;) urllib.request.urlretrieve(housing_url, tgz_path) housing_tgz = tarfile.open(tgz_path) housing_tgz.extractall(path=housing_path) housing_tgz.close() . fetch_housing_data() . import pandas as pd def load_housing_data(housing_path=HOUSING_PATH): csv_path = os.path.join(housing_path, &quot;housing.csv&quot;) return pd.read_csv(csv_path) . Preview of First 10 Rows&#182; . housing = load_housing_data() print(f&#39;Full dataset has {housing.shape[0]} rows, {housing.shape[0]} columns&#39;) . Full dataset has 20640 rows, 20640 columns . housing.head(10) . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity . 0 -122.23 | 37.88 | 41.0 | 880.0 | 129.0 | 322.0 | 126.0 | 8.3252 | 452600.0 | NEAR BAY | . 1 -122.22 | 37.86 | 21.0 | 7099.0 | 1106.0 | 2401.0 | 1138.0 | 8.3014 | 358500.0 | NEAR BAY | . 2 -122.24 | 37.85 | 52.0 | 1467.0 | 190.0 | 496.0 | 177.0 | 7.2574 | 352100.0 | NEAR BAY | . 3 -122.25 | 37.85 | 52.0 | 1274.0 | 235.0 | 558.0 | 219.0 | 5.6431 | 341300.0 | NEAR BAY | . 4 -122.25 | 37.85 | 52.0 | 1627.0 | 280.0 | 565.0 | 259.0 | 3.8462 | 342200.0 | NEAR BAY | . 5 -122.25 | 37.85 | 52.0 | 919.0 | 213.0 | 413.0 | 193.0 | 4.0368 | 269700.0 | NEAR BAY | . 6 -122.25 | 37.84 | 52.0 | 2535.0 | 489.0 | 1094.0 | 514.0 | 3.6591 | 299200.0 | NEAR BAY | . 7 -122.25 | 37.84 | 52.0 | 3104.0 | 687.0 | 1157.0 | 647.0 | 3.1200 | 241400.0 | NEAR BAY | . 8 -122.26 | 37.84 | 42.0 | 2555.0 | 665.0 | 1206.0 | 595.0 | 2.0804 | 226700.0 | NEAR BAY | . 9 -122.25 | 37.84 | 52.0 | 3549.0 | 707.0 | 1551.0 | 714.0 | 3.6912 | 261100.0 | NEAR BAY | . from pandas_profiling import ProfileReport profile = ProfileReport(housing, title=&#39;Pandas Profiling Report&#39;, minimal=True) . profile.to_file(output_file=&quot;your_report.html&quot;) . housing.describe() . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value . count 20640.000000 | 20640.000000 | 20640.000000 | 20640.000000 | 20433.000000 | 20640.000000 | 20640.000000 | 20640.000000 | 20640.000000 | . mean -119.569704 | 35.631861 | 28.639486 | 2635.763081 | 537.870553 | 1425.476744 | 499.539680 | 3.870671 | 206855.816909 | . std 2.003532 | 2.135952 | 12.585558 | 2181.615252 | 421.385070 | 1132.462122 | 382.329753 | 1.899822 | 115395.615874 | . min -124.350000 | 32.540000 | 1.000000 | 2.000000 | 1.000000 | 3.000000 | 1.000000 | 0.499900 | 14999.000000 | . 25% -121.800000 | 33.930000 | 18.000000 | 1447.750000 | 296.000000 | 787.000000 | 280.000000 | 2.563400 | 119600.000000 | . 50% -118.490000 | 34.260000 | 29.000000 | 2127.000000 | 435.000000 | 1166.000000 | 409.000000 | 3.534800 | 179700.000000 | . 75% -118.010000 | 37.710000 | 37.000000 | 3148.000000 | 647.000000 | 1725.000000 | 605.000000 | 4.743250 | 264725.000000 | . max -114.310000 | 41.950000 | 52.000000 | 39320.000000 | 6445.000000 | 35682.000000 | 6082.000000 | 15.000100 | 500001.000000 | . housing[&quot;ocean_proximity&quot;].value_counts() . &lt;1H OCEAN 9136 INLAND 6551 NEAR OCEAN 2658 NEAR BAY 2290 ISLAND 5 Name: ocean_proximity, dtype: int64 . %matplotlib inline import matplotlib.pyplot as plt housing.hist(bins=50, figsize=(20,15)) save_fig(&quot;attribute_histogram_plots&quot;) plt.show() . Saving figure attribute_histogram_plots . # to make this notebook&#39;s output identical at every run np.random.seed(42) . import numpy as np # For illustration only. Sklearn has train_test_split() def split_train_test(data, test_ratio): shuffled_indices = np.random.permutation(len(data)) test_set_size = int(len(data) * test_ratio) test_indices = shuffled_indices[:test_set_size] train_indices = shuffled_indices[test_set_size:] return data.iloc[train_indices], data.iloc[test_indices] . train_set, test_set = split_train_test(housing, 0.2) . from zlib import crc32 def test_set_check(identifier, test_ratio): return crc32(np.int64(identifier)) &amp; 0xffffffff &lt; test_ratio * 2**32 def split_train_test_by_id(data, test_ratio, id_column): ids = data[id_column] in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio)) return data.loc[~in_test_set], data.loc[in_test_set] . The implementation of test_set_check() above works fine in both Python 2 and Python 3. In earlier releases, the following implementation was proposed, which supported any hash function, but was much slower and did not support Python 2: . import hashlib def test_set_check(identifier, test_ratio, hash=hashlib.md5): return hash(np.int64(identifier)).digest()[-1] &lt; 256 * test_ratio . If you want an implementation that supports any hash function and is compatible with both Python 2 and Python 3, here is one: . def test_set_check(identifier, test_ratio, hash=hashlib.md5): return bytearray(hash(np.int64(identifier)).digest())[-1] &lt; 256 * test_ratio . housing_with_id = housing.reset_index() # adds an `index` column train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, &quot;index&quot;) . housing_with_id[&quot;id&quot;] = housing[&quot;longitude&quot;] * 1000 + housing[&quot;latitude&quot;] train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, &quot;id&quot;) . Median Income&#182; . housing[&quot;median_income&quot;].hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x133e082b0&gt; . housing[&quot;income_cat&quot;] = pd.cut(housing[&quot;median_income&quot;], bins=[0., 1.5, 3.0, 4.5, 6., np.inf], labels=[1, 2, 3, 4, 5]) . Geographic Distribution&#182; . import matplotlib.image as mpimg california_img=mpimg.imread(os.path.join(images_path, filename)) ax = housing.plot(kind=&quot;scatter&quot;, x=&quot;longitude&quot;, y=&quot;latitude&quot;, figsize=(10,7), s=housing[&#39;population&#39;]/100, label=&quot;Population&quot;, c=&quot;median_house_value&quot;, cmap=plt.get_cmap(&quot;jet&quot;), colorbar=False, alpha=0.4, ) plt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5, cmap=plt.get_cmap(&quot;jet&quot;)) plt.ylabel(&quot;Latitude&quot;, fontsize=14) plt.xlabel(&quot;Longitude&quot;, fontsize=14) prices = housing[&quot;median_house_value&quot;] tick_values = np.linspace(prices.min(), prices.max(), 11) cbar = plt.colorbar() cbar.ax.set_yticklabels([&quot;$%dk&quot;%(round(v/1000)) for v in tick_values], fontsize=14) cbar.set_label(&#39;Median House Value&#39;, fontsize=16) plt.legend(fontsize=16) #save_fig(&quot;california_housing_prices_plot&quot;) plt.show() . Correlation with median_house_value&#182; . corr_matrix[&quot;median_house_value&quot;].sort_values(ascending=False) . median_house_value 1.000000 median_income 0.687160 total_rooms 0.135097 housing_median_age 0.114110 households 0.064506 total_bedrooms 0.047689 population -0.026920 longitude -0.047432 latitude -0.142724 Name: median_house_value, dtype: float64 . Scatterplot Matrix&#182; . # from pandas.tools.plotting import scatter_matrix # For older versions of Pandas from pandas.plotting import scatter_matrix attributes = [&quot;median_house_value&quot;, &quot;median_income&quot;, &quot;total_rooms&quot;, &quot;housing_median_age&quot;] scatter_matrix(housing[attributes], figsize=(12, 8)) #save_fig(&quot;scatter_matrix_plot&quot;) plt.show() . Saving figure scatter_matrix_plot . Median House Value vs. Median Income&#182; . housing.plot(kind=&quot;scatter&quot;, x=&quot;median_income&quot;, y=&quot;median_house_value&quot;, alpha=0.1) plt.axis([0, 16, 0, 550000]) #save_fig(&quot;income_vs_house_value_scatterplot&quot;) plt.show() . housing[&quot;rooms_per_household&quot;] = housing[&quot;total_rooms&quot;]/housing[&quot;households&quot;] housing[&quot;bedrooms_per_room&quot;] = housing[&quot;total_bedrooms&quot;]/housing[&quot;total_rooms&quot;] housing[&quot;population_per_household&quot;]=housing[&quot;population&quot;]/housing[&quot;households&quot;] . Scatterplot Rooms Per Household vs. Median House Value&#182; . housing.plot(kind=&quot;scatter&quot;, x=&quot;rooms_per_household&quot;, y=&quot;median_house_value&quot;, alpha=0.2) plt.axis([0, 5, 0, 520000]) plt.show() .",
    "url": "https://machine-learning-apps.github.io/actions-ml-cicd/docs/reports/EDA/",
    "relUrl": "/docs/reports/EDA/"
  }
  ,"3": {
    "title": "Evaluate Model",
    "content": "from tensorflow.keras.models import load_model from utils import plot_confusion_matrix #load the test data and labels test_body_vecs = np.load(&#39;test_body_vecs.npy&#39;) test_title_vecs = np.load(&#39;test_title_vecs.npy&#39;) test_labels = np.load(&#39;test_labels.npy&#39;) #load the best model best_model = load_model(&#39;Issue_Label_v1_best_model.hdf5&#39;) #get predictions y_pred = np.argmax(best_model.predict(x=[test_body_vecs, test_title_vecs], batch_size=15000), axis=1) # get labels y_test = test_labels[:, 0] . plot_confusion_matrix(y_test, y_pred, classes=np.array([&#39;bug&#39;, &#39;feature&#39;, &#39;question&#39;]), normalize=True, title=&#39;Normalized Confusion Matrix&#39;) . Normalized confusion matrix [[0.88173203 0.09765211 0.02061586] [0.1303451 0.83997974 0.02967516] [0.27873486 0.23896011 0.48230502]] . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f487d90af60&gt; . Make Predictions&#182; . from utils import IssueLabeler from tensorflow.keras.models import load_model import dill as dpickle . #load the best model best_model = load_model(&#39;Issue_Label_v1_best_model.hdf5&#39;) #load the pre-processors with open(&#39;title_pp.dpkl&#39;, &#39;rb&#39;) as f: title_pp = dpickle.load(f) with open(&#39;body_pp.dpkl&#39;, &#39;rb&#39;) as f: body_pp = dpickle.load(f) # instantiate the IssueLabeler object issue_labeler = IssueLabeler(body_text_preprocessor=body_pp, title_text_preprocessor=title_pp, model=best_model) . Using TensorFlow backend. . issue_labeler.get_probabilities(body=&#39;Can someone please help me?&#39;, title=&#39;random stuff&#39;) . {&#39;bug&#39;: 0.12618249654769897, &#39;feature&#39;: 0.1929263472557068, &#39;question&#39;: 0.6808911561965942} . issue_labeler.get_probabilities(body=&#39;It would be great to add a new button&#39;, title=&#39;requesting a button&#39;) . {&#39;bug&#39;: 0.019261939451098442, &#39;feature&#39;: 0.9305700659751892, &#39;question&#39;: 0.05016808584332466} . issue_labeler.get_probabilities(body=&#39;It does` not work, I get bad errors&#39;, title=&#39;nothing works&#39;) . {&#39;bug&#39;: 0.9065071940422058, &#39;feature&#39;: 0.03202613815665245, &#39;question&#39;: 0.06146678701043129} .",
    "url": "https://machine-learning-apps.github.io/actions-ml-cicd/docs/reports/Model_Eval/",
    "relUrl": "/docs/reports/Model_Eval/"
  }
  ,"4": {
    "title": "Test Report",
    "content": "import scipy .",
    "url": "https://machine-learning-apps.github.io/actions-ml-cicd/docs/reports/cool_stuff/",
    "relUrl": "/docs/reports/cool_stuff/"
  }
  ,"5": {
    "title": "Data Dictionary",
    "content": "Overview Dataset Information | Variable Types | Warnings | | Overview . Dataset Information .     . Number of Variables | 17 | . Number of Observations | 4521 | . Total Missing | 1.1% | . Total Size in Memory | 600.5 KiB | . Average Record Size in Memory | 136 B | . Variable Types .     . Numeric | 7 | . Categorical | 8 | . Boolean | 2 | . Date | 0 | . Text (Unique) | 0 | . Rejected | 0 | . Unsupported | 0 | . Warnings . 7.59% zeros | . URL: http://data.data.com/data.csv .",
    "url": "https://machine-learning-apps.github.io/actions-ml-cicd/docs/data_dictionary/",
    "relUrl": "/docs/data_dictionary/"
  }
  ,"6": {
    "title": "Home",
    "content": "A MLOps Dashboard Automatically Created With GitHub Pages . This dashboard renders information in your repo relevant to machine learning projects automatically by using GitHub Actions and GitHub Pages. . Learn more Sections: . Summary: uses the /project/model_card.md file located your repo. This is meant to be a high level summary of your model. A recommended format for this is discussed in Model Cards for Model Reporting. . | Reports: these pages automatically render Jupyter notebooks located in the /notebooks directory of your repo. . | Model Timeline: this is a timeline view of relevant milestones for your ML project. This page is generated by /project/model_timeline.csv in your repo, which is system generated 1. . | Completed Pipeline Runs: This shows the most recent pipeline runs. This page is generated by /metadata/pipeline_runs.json in our repo, which is system generated 1. . | Deploy: A click-to-deploy mechanism for others to run your training pipeline and serve your model on the infrastructure of your choice. This is under construction. . | . . This system has not been designed yet. &#8617; &#8617;2 . |",
    "url": "https://machine-learning-apps.github.io/actions-ml-cicd/",
    "relUrl": "/"
  }
  ,"7": {
    "title": "Reports",
    "content": "Reports are automatically rendered from Jupyter Notebooks located in the /notebooks directory of your repository. You can refresh these reports by commiting new or updated notebooks to the master branch of your repository. .",
    "url": "https://machine-learning-apps.github.io/actions-ml-cicd/docs/reports/",
    "relUrl": "/docs/reports/"
  }
  ,"8": {
    "title": "Summary",
    "content": "Model Details | Intended Use | Factors | Metrics | Training Data | Ethical Considerations | Caveats and Recommendations | Model Details . Developed by researchers at Google and the University of Toronto, 2018, v1. | Convolutional Neural Net. | Pretrained for face recognition then fine-tuned with cross-entropy loss for binary smiling classification. | . Intended Use . Intended to be used for fun applications, such as creating cartoon smiles on real images; augmentative applications, such as providing details for people who are blind; or assisting applications such as automatically finding smiling photos. | Particularly intended for younger audiences. | Not suitable for emotion detection or determining affect; smiles were annotated based on physical appearance, and not underlying emotions. | . Factors . Based on known problems with computer vision face technology, potential relevant factors include groups for gender, age, race, and Fitzpatrick skin type; hardware factors of camera type and lens type; and environmental factors oflighting and humidity. | Evaluation factors are gender and age group, as annotated in the publicly available dataset CelebA [36]. Further possible factors not currently available in a public smiling dataset. Gender and age determined by third-party annotators based on visual presentation, following a set of examples of male/female gender and young/old age. Further details available in [36]. | . Metrics . Evaluation metrics include False Positive Rate and False Negative Rate to measure disproportionate model performance errors across subgroups. False Discovery Rate and False Omission Rate, which measure the fraction of negative (not smiling) and positive (smiling) predictions that are incorrectly predicted to be positive and negative, respectively are also reported. [48] | Together, these four metrics provide values for different errors that can be calculated from the confusion matrix for binary classification systems. | These also correspond to metrics in recent definitions of “fairness” in machine learning (cf. [6, 26]), where parity across subgroups for different metrics correspond to different fairness criteria. | 95% confidence intervals calculated with bootstrap resampling. | All metrics reported at the .5 decision threshold, where all error types (FPR, FNR, FDR, FOR) are within the same range (0.04 - 0.14). | . Training Data . CelebA [36], training data split. Evaluation Data | CelebA [36], test data split. | Chosen as a basic proof-of-concept. | . Ethical Considerations . Faces and annotations based on public figures (celebrities). No new information is inferred or annotated. | . Caveats and Recommendations . Does not capture race or skin type, which has been reported as a source of disproportionate errors [5]. | Given gender classes are binary (male/not male), which we include as male/female. Further work needed to evaluate across a spectrum of genders. | An ideal evaluation dataset would additionally include annotations for Fitzpatrick skin type, camera details, and environment (lighting/humidity) details. | This is an example. | .",
    "url": "https://machine-learning-apps.github.io/actions-ml-cicd/summary",
    "relUrl": "/summary"
  }
  ,"9": {
    "title": "Model Timeline",
    "content": "increased accuracy by 2%, commitFri Jan 29 10:43:05 EST 2020 . 601423b . deploy, deployed a model to productionFri Jan 29 11:43:05 EST 2020 . demo-mlops-2 . experiments, ran 25 experimentsFri Jan 31 1:43:05 EST 2020 . WandB.com . increased accuracy by 2%, commitFri Feb 02 12:20:10 EST 2020 . 601423b . increased accuracy by 1.1%, commitFri Feb 08 12:20:10 EST 2020 . 601423b .",
    "url": "https://machine-learning-apps.github.io/actions-ml-cicd/docs/model_timeline/",
    "relUrl": "/docs/model_timeline/"
  }
  
}