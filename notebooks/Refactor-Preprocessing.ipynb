{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ktext.preprocess import processor\n",
    "import dill as dpickle\n",
    "import numpy as np\n",
    "import h5py\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_csv(f'https://storage.googleapis.com/codenet/issue_labels/00000000000{i}.csv.gz')\n",
    "                for i in range(1)])\n",
    "\n",
    "#split data into train/test\n",
    "traindf, testdf = train_test_split(df, test_size=.15, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 270,624 rows 10 columns\n",
      "Test: 47,758 rows 10 columns\n",
      " Avg # of issues per repo: 2.6\n",
      " Avg # of issues per org: 2.8\n",
      "                                pcnt  count\n",
      "repo                                       \n",
      "Microsoft/vscode            0.005145   1638\n",
      "rancher/rancher             0.002349    748\n",
      "MicrosoftDocs/azure-docs    0.002060    656\n",
      "godotengine/godot           0.001894    603\n",
      "ansible/ansible             0.001866    594\n",
      "hashicorp/terraform         0.001624    517\n",
      "kubernetes/kubernetes       0.001504    479\n",
      "lionheart/openradar-mirror  0.001432    456\n",
      "dart-lang/sdk               0.001159    369\n",
      "elastic/kibana              0.001156    368\n",
      "eclipse/che                 0.001150    366\n",
      "dotnet/corefx               0.001146    365\n",
      "magento/magento2            0.001040    331\n",
      "brave/browser-laptop        0.001027    327\n",
      "kbower/tickettest1          0.000974    310\n",
      "Kademi/kademi-dev           0.000832    265\n",
      "eslint/eslint               0.000801    255\n",
      "owncloud/core               0.000782    249\n",
      "openshift/origin            0.000773    246\n",
      "elastic/elasticsearch       0.000766    244\n"
     ]
    }
   ],
   "source": [
    "# Descriptive Statistics\n",
    "print(f'Train: {traindf.shape[0]:,} rows {traindf.shape[1]:,} columns')\n",
    "print(f'Test: {testdf.shape[0]:,} rows {testdf.shape[1]:,} columns')\n",
    "traindf.groupby('class_int').size()\n",
    "print(f' Avg # of issues per repo: {len(traindf) / traindf.repo.nunique():.1f}')\n",
    "print(f\" Avg # of issues per org: {len(traindf) / traindf.repo.apply(lambda x: x.split('/')[-1]).nunique():.1f}\")\n",
    "pareto_df = pd.DataFrame({'pcnt': df.groupby('repo').size() / len(df), 'count': df.groupby('repo').size()})\n",
    "print(pareto_df.sort_values('pcnt', ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:Setting maximum document length to 90 based upon hueristic of 0.75 percentile.\n",
      " See full histogram by insepecting the `document_length_stats` attribute.\n",
      "WARNING:root:(1/2) done. 113 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 12 sec\n",
      "WARNING:root:Finished parsing 270,624 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 7 sec\n",
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:Setting maximum document length to 10 based upon hueristic of 0.75 percentile.\n",
      " See full histogram by insepecting the `document_length_stats` attribute.\n",
      "WARNING:root:(1/2) done. 11 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 1 sec\n",
      "WARNING:root:Finished parsing 270,624 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 1 sec\n"
     ]
    }
   ],
   "source": [
    "# Clean, tokenize, and apply padding / truncating such that each document length = 75th percentile for the dataset.\n",
    "#  also, retain only the top keep_n words in the vocabulary and set the remaining words\n",
    "#  to 1 which will become common index for rare words \n",
    "\n",
    "train_body_raw = traindf.body.tolist()\n",
    "train_title_raw = traindf.title.tolist()\n",
    "\n",
    "# process the issue body data\n",
    "body_pp = processor(.75, keep_n=8000)\n",
    "train_body_vecs = body_pp.fit_transform(train_body_raw)\n",
    "\n",
    "# process the title data\n",
    "title_pp = processor(.75, keep_n=4500)\n",
    "train_title_vecs = title_pp.fit_transform(train_title_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:...tokenizing data\n",
      "WARNING:root:...indexing data\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:...tokenizing data\n",
      "WARNING:root:...indexing data\n",
      "WARNING:root:...padding data\n"
     ]
    }
   ],
   "source": [
    "# apply transformations to test data\n",
    "test_body_raw = testdf.body.tolist()\n",
    "test_title_raw = testdf.title.tolist()\n",
    "\n",
    "test_body_vecs = body_pp.transform_parallel(test_body_raw)\n",
    "test_title_vecs = title_pp.transform_parallel(test_title_raw)\n",
    "\n",
    "\n",
    "# extract labels\n",
    "train_labels = np.expand_dims(traindf.class_int.values, -1)\n",
    "test_labels = np.expand_dims(testdf.class_int.values, -1)\n",
    "num_classes = len(set(train_labels[:, 0]))\n",
    "\n",
    "# Check shapes\n",
    "# the number of rows in data for the body, title and labels should be the same for both train and test partitions\n",
    "assert train_body_vecs.shape[0] == train_title_vecs.shape[0] == train_labels.shape[0]\n",
    "assert test_body_vecs.shape[0] == test_title_vecs.shape[0] == test_labels.shape[0]\n",
    "assert num_classes == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('data/dataset.hdf5', 'w')\n",
    "f.create_dataset('/titles', data=train_title_vecs)\n",
    "f.create_dataset('/bodies', data=train_body_vecs)\n",
    "f.create_dataset('/targets', data=train_labels)\n",
    "\n",
    "f.create_dataset('/test_titles', data=test_title_vecs)\n",
    "f.create_dataset('/test_bodies', data=test_body_vecs)\n",
    "f.create_dataset('/test_targets', data=test_labels)\n",
    "f.close()\n",
    "\n",
    "\n",
    "with open(\"data/metadata.json\", \"w\") as f:\n",
    "    meta = {\n",
    "        'body_vocab_size': body_pp.n_tokens,\n",
    "        'title_vocab_size': title_pp.n_tokens,\n",
    "        'issue_body_doc_length': train_body_vecs.shape[1],\n",
    "        'issue_title_doc_length': train_title_vecs.shape[1],\n",
    "        'num_classes': num_classes,\n",
    "    }\n",
    "    f.write(json.dumps(meta))\n",
    "    \n",
    "# Save the preprocessor\n",
    "with open('data/body_pp.dpkl', 'wb') as f:\n",
    "    dpickle.dump(body_pp, f)\n",
    "\n",
    "with open('data/title_pp.dpkl', 'wb') as f:\n",
    "    dpickle.dump(title_pp, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 360960\r\n",
      "drwxr-xr-x   6 hamelsmu  staff   192B Oct  9 11:59 \u001b[1m\u001b[36m.\u001b[m\u001b[m\r\n",
      "drwxr-xr-x  11 hamelsmu  staff   352B Oct  9 12:02 \u001b[1m\u001b[36m..\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 hamelsmu  staff    35M Oct  9 12:02 body_pp.dpkl\r\n",
      "-rw-r--r--   1 hamelsmu  staff   124M Oct  9 12:02 dataset.hdf5\r\n",
      "-rw-r--r--   1 hamelsmu  staff   128B Oct  9 12:02 metadata.json\r\n",
      "-rw-r--r--   1 hamelsmu  staff   4.1M Oct  9 12:02 title_pp.dpkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lah data/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
